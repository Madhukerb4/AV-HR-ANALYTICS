{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the required Packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time\n",
    "import joblib\n",
    "# to display all columns of the dataframe in the notebook\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import statistics\n",
    "import scipy.stats as stats\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading pickled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54808, 46)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### loading pickled dataset ####\n",
    "df_train_fs = joblib.load(\"df_train_fs.pkl\")\n",
    "df_train_fs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'employee_id', 'service_yrs', 'kpi', 'awarded',\n",
       "       'avg_training_score', 'promoted', 'previous_rating_nan', 'f_age_bins',\n",
       "       'ohe_dept_Analytics', 'ohe_dept_Finance', 'ohe_dept_HR',\n",
       "       'ohe_dept_Legal', 'ohe_dept_Operations', 'ohe_dept_Procurement',\n",
       "       'ohe_dept_R&D', 'ohe_dept_Sales & Marketing', 'ohe_dept_Technology',\n",
       "       'f_dept_train_score_mean', 'f_dept_train_score_median',\n",
       "       'f_dept_train_score_min', 'f_dept_train_score_max', 'f_dept_age_median',\n",
       "       'f_dept_service_max', 'f_region_train_score_mean',\n",
       "       'f_region_train_score_median', 'f_region_train_score_min',\n",
       "       'f_region_train_score_max', 'f_region_train_score_std',\n",
       "       'f_region_age_median', 'f_region_age_min', 'f_region_age_max',\n",
       "       'f_region_service_mean', 'f_region_service_max', 'ohe_edu_Bachelor's',\n",
       "       'ohe_edu_Below Secondary', 'ohe_edu_Master's & above', 'f_edu_age_std',\n",
       "       'f_trainings_oneplus', 'f_rating_edu_cnt', 'f_rating_dept_cnt',\n",
       "       'f_dept_edu_cnt', 'f_dept_region_cnt', 'f_region_edu_cnt',\n",
       "       'f_region_rating_cnt', 'f_performance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### column names\n",
    "df_train_fs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50140\n",
       "1     4668\n",
       "Name: promoted, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target value count\n",
    "df_train_fs['promoted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24668, 46)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# undersampling technique\n",
    "np.random.seed(1234)\n",
    "df_undersample = df_train_fs[df_train_fs['promoted'] == 0].sample(20000)\n",
    "df_undersample = df_undersample.append(df_train_fs[df_train_fs['promoted'] == 1])\n",
    "df_undersample.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23812, 44)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop index and source_data column\n",
    "df_undersample = df_undersample.drop(labels = ['index', 'employee_id'], axis = 1)\n",
    "# removing duplicate records\n",
    "df_undersample = df_undersample.drop_duplicates(keep = 'first')\n",
    "df_undersample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model parameter optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.514887 using {'C': 10, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.490492 (0.009399) with: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 0.01, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.496868 (0.009168) with: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.498306 (0.019066) with: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.503133 (0.009422) with: {'C': 0.01, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.490492 (0.009399) with: {'C': 0.01, 'max_iter': 400, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 0.01, 'max_iter': 400, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 0.01, 'max_iter': 400, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.496868 (0.009168) with: {'C': 0.01, 'max_iter': 400, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.503628 (0.010041) with: {'C': 0.01, 'max_iter': 400, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.503133 (0.009422) with: {'C': 0.01, 'max_iter': 400, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.512723 (0.010436) with: {'C': 1.0, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 1.0, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 1.0, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.500326 (0.008267) with: {'C': 1.0, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.505610 (0.017487) with: {'C': 1.0, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.514545 (0.009766) with: {'C': 1.0, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.512723 (0.010436) with: {'C': 1.0, 'max_iter': 400, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 1.0, 'max_iter': 400, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 1.0, 'max_iter': 400, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.500326 (0.008267) with: {'C': 1.0, 'max_iter': 400, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.507033 (0.011606) with: {'C': 1.0, 'max_iter': 400, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.514545 (0.009766) with: {'C': 1.0, 'max_iter': 400, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.512185 (0.010062) with: {'C': 5, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 5, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 5, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.499494 (0.011088) with: {'C': 5, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.508681 (0.018762) with: {'C': 5, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.514411 (0.009451) with: {'C': 5, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.512185 (0.010062) with: {'C': 5, 'max_iter': 400, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 5, 'max_iter': 400, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 5, 'max_iter': 400, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.499494 (0.011088) with: {'C': 5, 'max_iter': 400, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.505808 (0.005468) with: {'C': 5, 'max_iter': 400, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.514411 (0.009451) with: {'C': 5, 'max_iter': 400, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.508830 (0.010998) with: {'C': 10, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 10, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 10, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.499401 (0.014843) with: {'C': 10, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.504643 (0.019541) with: {'C': 10, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.514887 (0.009071) with: {'C': 10, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.508830 (0.010998) with: {'C': 10, 'max_iter': 400, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.000000 (0.000000) with: {'C': 10, 'max_iter': 400, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
      "0.000000 (0.000000) with: {'C': 10, 'max_iter': 400, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
      "0.499401 (0.014843) with: {'C': 10, 'max_iter': 400, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.502907 (0.007978) with: {'C': 10, 'max_iter': 400, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.514887 (0.009071) with: {'C': 10, 'max_iter': 400, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Wall time: 23min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### grid search for logistic regression ####\n",
    "#### identifying hyperparameters of the logistic regression Model #####\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define model and parameters\n",
    "logit_r = LogisticRegression(random_state= 0)\n",
    "solvers = ['liblinear', 'lbfgs', 'newton-cg']\n",
    "penalty = ['l1', 'l2']\n",
    "c_values = [0.01, 1.0, 5, 10]\n",
    "max_iter = [200, 400]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,max_iter=max_iter, penalty=penalty,C=c_values)  \n",
    "\n",
    "# StratifiedKfold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "strkfold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=logit_r, param_grid=grid, n_jobs=-1, cv=strkfold, scoring='f1',error_score=0)\n",
    "grid_result = grid_search.fit(df_undersample.drop(labels = ['promoted'], axis = 1), df_undersample['promoted'])\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model with Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Logistic Regression Model ####\n",
    "logit_r = LogisticRegression(C= 10, max_iter = 200, penalty= 'l2', solver= 'newton-cg', random_state= 0 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Classifier Model Parameter Optimizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameter optimizing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {\n",
    "              \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "              \"min_child_weight\": [5, 8],\n",
    "              \"gamma\" : [0],\n",
    "              \"reg_alpha\": [0.01, 0.1, 1],\n",
    "              \"subsample\": [0.5, 0.8],\n",
    "              'n_estimators': [400, 600],\n",
    "              'max_depth' : [5, 8],\n",
    "              \"colsample_bytree\" : [0.8],\n",
    "              \"eval_metric\" : ['error']\n",
    "              }\n",
    "\n",
    "# StratifiedKfold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "strkfold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "### Creation of Base model ###\n",
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier(booster = 'gbtree', random_state = 0 )\n",
    "\n",
    "# Grid Search \n",
    "grid_search = GridSearchCV(estimator = xgbc, param_grid = param_grid, cv = strkfold, scoring='f1', error_score=0, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 92.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 188.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 321.9min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 366.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6h 7min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             error_score=0,\n",
       "             estimator=XGBClassifier(base_score=None, booster='gbtree',\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_w...\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.8], 'eval_metric': ['error'],\n",
       "                         'gamma': [0], 'learning_rate': [0.05, 0.1, 0.2],\n",
       "                         'max_depth': [5, 8], 'min_child_weight': [5, 8],\n",
       "                         'n_estimators': [400, 600],\n",
       "                         'reg_alpha': [0.01, 0.1, 1], 'subsample': [0.5, 0.8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fitting the grid_search to the model ####\n",
    "grid_search.fit(df_undersample.drop(labels = ['promoted'], axis = 1), df_undersample['promoted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'eval_metric': 'error',\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.05,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 8,\n",
       " 'n_estimators': 400,\n",
       " 'reg_alpha': 0.1,\n",
       " 'subsample': 0.8}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search Best Params\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Model with Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit xgboost model with optimized parameters \n",
    "from xgboost import XGBClassifier\n",
    "xgbc = XGBClassifier(random_state = 0, colsample_bytree = 0.8, eval_metric = 'error', gamma = 0,  learning_rate = 0.05, max_depth = 8,\n",
    "                      min_child_weight = 8, n_estimators = 400, reg_alpha = 0.1,  subsample = 0.8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM Classifier Model Parameter Optimizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### identifying hyperparameters for light gbm #####\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "# parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [100, 200] ,\n",
    "    'n_estimators': [400, 600, 800, 1000] ,\n",
    "    'max_depth':[5, 10],\n",
    "    \"importance_type\":['split', 'gain'],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    'subsample_for_bin' : [8000, 16000],\n",
    "    'min_child_samples' : [100, 200],\n",
    "    'colsample_bytree' : [0.6, 0.8],\n",
    "    'reg_alpha' : [0.001, 0.01, 0.1]\n",
    "        }\n",
    "# StratifiedKfold\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "strkfold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "### Creation of Base model ###\n",
    "import lightgbm as lgbm\n",
    "lightgbm = lgbm.LGBMClassifier(boosting_type= 'goss', objective= 'binary', random_state= 0 )\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = lightgbm, param_grid = param_grid, cv = strkfold,  scoring='f1', n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2304 candidates, totalling 11520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 36.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed: 72.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed: 95.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed: 122.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 155.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4893 tasks      | elapsed: 189.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5824 tasks      | elapsed: 227.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6837 tasks      | elapsed: 272.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7930 tasks      | elapsed: 321.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9105 tasks      | elapsed: 371.4min\n",
      "[Parallel(n_jobs=-1)]: Done 10360 tasks      | elapsed: 435.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11520 out of 11520 | elapsed: 485.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8h 6min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             error_score=nan,\n",
       "             estimator=LGBMClassifier(boosting_type='goss', class_weight=None,\n",
       "                                      colsample_bytree=1.0,\n",
       "                                      importance_type='split',\n",
       "                                      learning_rate=0.1, max_depth=-1,\n",
       "                                      min_child_samples=20,\n",
       "                                      min_child_weight=0.001,\n",
       "                                      min_split_gain=0.0, n_estimators=100,\n",
       "                                      n_jobs=-1, num_leaves=31,\n",
       "                                      objective='binary'...\n",
       "             param_grid={'colsample_bytree': [0.6, 0.8],\n",
       "                         'importance_type': ['split', 'gain'],\n",
       "                         'learning_rate': [0.05, 0.1, 0.2],\n",
       "                         'max_depth': [5, 10], 'min_child_samples': [100, 200],\n",
       "                         'n_estimators': [400, 600, 800, 1000],\n",
       "                         'num_leaves': [100, 200],\n",
       "                         'reg_alpha': [0.001, 0.01, 0.1],\n",
       "                         'subsample_for_bin': [8000, 16000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#### Fitting the grid_search to the model ####\n",
    "grid_search.fit(df_undersample.drop(labels = ['promoted'], axis = 1), df_undersample['promoted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.05,\n",
       " 'max_depth': 10,\n",
       " 'min_child_samples': 200,\n",
       " 'n_estimators': 1000,\n",
       " 'num_leaves': 100,\n",
       " 'reg_alpha': 0.1,\n",
       " 'subsample_for_bin': 16000}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting the Best Estimator #####\n",
    "grid_search.best_params_\n",
    "#{'colsample_bytree': 0.8,  'importance_type': 'split', 'learning_rate': 0.05, 'max_depth': 10, 'min_child_samples': 200,\n",
    " # n_estimators': 1000,  'num_leaves': 100, 'reg_alpha': 0.1, 'subsample_for_bin': 16000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM Model with Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### implementation of light gbm #####\n",
    "import lightgbm as lgbm\n",
    "lightgbm = lgbm.LGBMClassifier(boosting_type= 'goss', objective= 'binary', random_state= 0, importance_type = 'split', colsample_bytree = 0.05, max_depth= 10, min_child_samples = 200, n_estimators= 1000, num_leaves= 100, reg_alpha= 0.1, subsample_for_bin= 16000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble voting classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "models_list = []\n",
    "models_list.append(('Logistic Regression', logit_r))\n",
    "models_list.append(('XGBoost', xgbc))\n",
    "models_list.append(('LightGBM', lightgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('Logistic Regression',\n",
       "                              LogisticRegression(C=10, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=200,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=0,\n",
       "                                                 solver='newton-cg', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('XGBoost',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            colsampl...\n",
       "                                             importance_type='split',\n",
       "                                             learning_rate=0.1, max_depth=10,\n",
       "                                             min_child_samples=200,\n",
       "                                             min_child_weight=0.001,\n",
       "                                             min_split_gain=0.0,\n",
       "                                             n_estimators=1000, n_jobs=-1,\n",
       "                                             num_leaves=100, objective='binary',\n",
       "                                             random_state=0, reg_alpha=0.1,\n",
       "                                             reg_lambda=0.0, silent=True,\n",
       "                                             subsample=1.0,\n",
       "                                             subsample_for_bin=16000,\n",
       "                                             subsample_freq=0))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# voting classification fit\n",
    "ensemble_m = VotingClassifier(estimators= models_list, voting= 'soft')\n",
    "ensemble_m.fit(df_undersample.drop(labels = ['promoted'], axis = 1), df_undersample['promoted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model \n",
    "joblib.dump(logit_r, \"hr_logistic_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier model ####\n",
    "joblib.dump(xgbc, \"xgboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM Classifier model ####\n",
    "joblib.dump(lightgbm, \"lightgbm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Voting_class_ensemlbe_model.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting Classification Ensemble \n",
    "joblib.dump(ensemble_m, \"Voting_class_ensemlbe_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kindly comment on the approach for improvement of the model & learning!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
